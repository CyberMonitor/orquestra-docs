---
title: "Recurrent Neural Networks with LSTM"
summary: Predict a noisy sine function by means of Recurrent Neural Networks.
weight: 4
publishdate: 2099-01-01
---

This tutorial will walk through the implementation of a neural network model that can learn how to reconstruct the shape of a noisy sine function using *Recurrent Neural Networks (RNNs)*.

RNNs are one of the canonical approaches for dealings with sequence problems since they can retain the state from one iteration to the next by using their own output as additional input for the next step, i.e., the input for RNNs correspond to the data and the state of the network one previous step. Simple RNNs suffer from the fundamental problem of not being able to capture long-term dependencies in a sequence, and to tackle that issue in the late '90s, the so-called Long Short-Term Memory networks (LSTMs) were proposed by [**Sepp Hochreiter** and **Jurgen Schmidhuber**](https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735). This exercise will make use of LSTMs as neurons type in the model.

The problem considered here belongs to the category of *supervised learning* since it involves learning a function that maps a set of inputs to one output based on multiple instances of (input, output) pairs. In the following section, it is stated the input and output datasets of the problem at hand.


## Input and output dataset
The raw dataset consists of a *noisy sine function* data, generated by adding up an analytical sine function and a noisy signal sampled from a normal distribution with mean zero and standard deviation of one. The parameters that control the data generation are the following:
- *time-range*: the upper limit of the time range of data to be generated. The time range starts at 0. The `time_range` is not included as the last point.
- *time-step*: the step between each of the time values.
- *noise-std*: the standard deviation of the noise. As mentioned above, the noise follows a normal distribution centered at zero.

The generated data looks like the plot below:
    
![](../../img/tutorials/SineFunc.png)

As explained later, this raw data will be split into training and testing datasets, and both of them will be subjected to an operation that creates the pair values of (input, output) of the supervised problem. In this respect, the x-values (inputs) correspond to a time rolling window of specified length, and the y-values (outputs) to the next value to be predicted. Below there is a snippet of the code realizing that operation.

The series of (input, output) pairs for supervising learning are built in the following manner:
- Select a *lookback window* value, which will represent the number of previous time steps values that will be used as information for predicting the next value.
- Create x-values: split up the data in batches of size *lookback window* above in a rolling window manner across all available data.
- Create y-values: pick up the next time step value data for each of the batches of size *lookback window* above.

You can find a snippet of the code to generate the series of (input, output) pairs.

```python

  # Create pairs of a window of data and the next value after the window
  xs, ys = [], []
  for i in range(len(x) - window_size):
      v = x.iloc[i:(i + window_size)].values
      xs.append(v)
      ys.append(y.iloc[i + window_size])

    return np.array(xs), np.array(ys)
```

## Neural network model
RNNs model may be considered a computational black box that takes in a number of sequential values within a lookback window of values and forecasts the next value. In this exercise, the type of RNN used is that of LSTM, which is explicitly designed to capture long-term dependencies in sequential problems. Below there is an illustration of the computation that each LSTM represents.

![](../../img/tutorials/RNN_Rollout.png)



where the **x**'s represent the set of values of size *lookback window* that corresponds to the input data, and the **h**'s are the state values of the neurons in the RNN layer, which summarize the information from all the previous steps within the lookback window seen thus far. Please note that, as is standard practice, the initialization of the hidden layer output is set up to zero, i.e., `h0 = 0`. Last, **y** represents the output data to be predicted.


Therefore, the idea of an RNN model as a computational black box can be substantiated by looking at the NN model as a machine accepting a series of inputs, **x**'s, to produce an output **y**.

The model here used consists of a neural network with a single hidden layer of *LSTM* neurons and a final *Dense* layer for the output, where *Dense* in this context means that all neurons from the previous layer are connected to the current one. In this case, the output layer with one single neuron, which corresponds to the **y** predicted value. Additionally, to prevent overfitting in the training process, a [**Dropout**](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) operation is added to the LSTM layer, which can be seen as a form of regularization.

The code to create the model reads as follows:

```python
# Function to build the model
def build_model(hnodes=32, dropout=0.2):
    model = keras.Sequential()

    # Adding one LSTM layer
    model.add(keras.layers.LSTM(
        units=hnodes,
        input_shape=(xtrain.shape[1], xtrain.shape[2])
    ))

    # Adding dropout
    model.add(keras.layers.Dropout(dropout))

    # Adding a Dense layer at the end
    model.add(keras.layers.Dense(units=1))

    # Compile model using MSE as loss function to minimize, and Adam as optimiser
    model.compile(
        loss='mean_squared_error',
        optimizer=keras.optimizers.Adam(learning_rate)
    )
```

After having created the model, it is possible to print out its main characteristics, such as layers and the number of their corresponding parameters. The table shown below can be generated with the Keras command `{your_model_name}.summary()`, which summarizes the model:

![](../../img/tutorials/Model_Summary.png)

It is also possible to plot the model by means of importing package `from tensorflow.keras.utils import plot_model`, and by using the command `plot_model({your_model_name}, to_file='{plot_name}.png', `
`how_shapes=True, show_layer_names=False)`. The output for this model is shown below:

![](../../img/tutorials/ModelPlot.png)

The meaning of the values in parenthesis is the following:
- `(?, 10, 1)`: the first location corresponds to the size of the training dataset, and `?` states that the number is open, and does not need to be explicitly stated; the second position is the *lookback window*, which in this problem was set up to 10; finally, the third position means the number of input sequences, which it is just 1 in this problem.
- `(?, 128)`: the first position corresponds, as above, to the size of the training dataset; and the second position to the number of neurons in the layer, i.e., 128 LSTM neurons on this hidden layer of the problem.
- `(?, 1))`: the first position, as above, is the size of the training dataset; and the second corresponds to the number of output neurons, in this case just 1, as the model predicts only one value, i.e., the next time step value.

Now, it can be stated how the RNN model can be used to solve the supervised learning problem in hand:
- Initialize the values of all the parameters involved in the RNN model.
- Use a value, **x**, from each of the values on the left side in (input, output) pairs in the dataset.
- Pass that input into the RNN model produces a *predicted* value, **y**, out of it.
- Compare the predicted **y** value from the **y** value from the right side in the pairs (input, output) in the dataset, and compute the error between them.
- Update the parameters of the model to minimize (optimize) the error calculated above.

Therefore, to complete the model, and to enable it to learn from data, it is required to specify a *Loss function*, which measures the deviation between the predicted values of the model and the ground truth values (the **y** values from the dataset pairs), and an *Optimizer* as the computational object to drive the search in parameter space aims to minimize that deviation. In the implementation for this problem, we use the *mean squared error (MSE)* as the loss function, and an extension of stochastic gradient descent, named [**Adam**](https://arxiv.org/pdf/1412.6980.pdf), as the optimizer. Below is the MSE loss function, and the snippet of the code to build the model.

![](../../img/tutorials/MSE_LossFunc.png)

As part of this tutorial, we save the model's architecture and weights as a JSON file. This allows us to use this model as an Orquestra artifact which can be later utilized a posteriori as a black-box predictor, or even as initialization condition for further training, as in the case of *transfer learning*, for instance.

## Test-train split

To train the model the full original dataset is first divided into training and testing sets. The first dataset will be used for training the model, and the second one to check the generalization capability of the model. In general terms:

- The *train dataset* is used to train the model, i.e., weights and biases in the LSTM layers. The model learns from this data.

- The *valuation dataset* is a percentage of the training dataset used to provide an unbiased evaluation of the model along the training process. It is a means to assess a potential overfitting of the model. This percentage value is a hyperparameter of the model, which in our example is initially set up to 0.1, i.e., 10% of the training data.

- The *test dataset* is used to provide an unbiased evaluation of the model. It is only used once a model is completely trained.

![](../../img/tutorials/TrainTest.png)


Below is a snippet of the code implementing the splitting operation of the raw data into training and test.

```python
# Splitting up dataset into training and testing datsets
#   trainperc: percentage of raw dataset to go to training set
#   dfsize: size of raw dataset
train_size = int(dfsize * trainperc)
test_size = dfsize - train_size
train, test = df.iloc[0:train_size], df.iloc[train_size:dfsize]
```

## Plotting prediction vs. test values
Below is a plot representing the training history, i.e., the loss function (MSE), defined at the time of model compilation above, both during validation and training stages. It can be noted that the validation error is less than the training error, which can be understood as the training error is calculated for the entire epoch - where at its beginning is worse than at the end -, whereas the validation error is taken from the last batch  - after the model is improved by the training process.

![](../../img/tutorials/CostFunction.png)


## Composing a workflow to generate an LSTM predictor model
In the next steps, it will be stated the code necessary to build and train this RNN model in Orquestra. The code consists of the following parts:
- `data_manipulator.py`: builds and preprocesses the data.
- `lstm_model.py`: builds and trains the network model.

**1. Create a GitHub repository**

Go to [GitHub](https://github.com/) and create a public repository called `lstm`. If you are unfamiliar with GitHub, you can reference the [create a repo guide](https://help.github.com/en/github/getting-started-with-github/create-a-repo) for help.

This repository will be where you build your component. [This GitHub repo](https://github.com/zapatacomputing/z-lstm) can be used as a reference for how the `lstm` component should look like throughout the tutorial.

**2. Add python code to the repository**

Now we will add the bulk of the workflow, which is the Python code that trains the model. For this purpose, using either the GitHub UI or by cloning your repo and using the command line, create two files: `src/python/lstm/data_manipulator.py` to generate and manipulate the input datasets, and `src/python/lstm/lstm_model.py` to build the network model. Their corresponding code is stated below:

- `data_manipulator.py`:

```python
import sys
import json
import numpy as np
import pandas as pd
from typing import TextIO

def noisy_sine_generation(time_range:float, time_step:float, noise_std:float) -> dict:
    """
    Generates noisy sine data.

    Args:
      time_range (float):
        The upper limit of the time range for the data to generate. The time
        range starts at 0. The time_range is not included as the last point.
      time_step (float):
        The step between each of the time values.
      noise_std (float):
        The standard deviation of the noise. Noise follows a normal distribution
        centered at zero.

    Returns:
      data_dict (dict):
        A dict containing a dict representation of a Pandas dataframe within its
        "data" field.
    """

    print('time_range = ', time_range)
    print('time_step = ', time_step)
    print('noise_std = ', noise_std)

    data_dict = {}

    try:
        time = np.arange(0, time_range, time_step)

        # Generating data: sine function
        values = np.sin(time) + np.random.normal(scale=noise_std, size=len(time))
        print('Values shape from numpy: ', values.shape)

        # Making pandas DataFrame
        data_df = pd.DataFrame(data=np.transpose([time, values]), columns=['time','values'])

        print('Data shape from pandas:')
        print(data_df.shape)
        print('DataFrame head:')
        print(data_df.head())

        # Save data in dict for serialization into JSON
        data_dict["data"] = data_df.to_dict()
    except Exception as e:
        e = sys.exc_info()[0]
        print(f'Error: {e}')

    return data_dict


def preprocess_data(data:dict, train_frac:float=0.8, window_size:int=10) -> (dict, dict, dict, dict):
    """
    Preprocesses data into a format suitable for training a model, splits it 
    into training and testing sets, and creates datasets of lookback windows and 
    next values.

    Args:
      data (dict):
        A dict with two keys, each containing indexes as keys and data as values 
        (this is the dict format of Pandas DataFrames). Here is an example:
        {
          "x": {
            "0": 0.0,
            "1": 0.1
          },
          "y": {
            "0": 1.0,
            "1": 2.0
          }
        }
      train_frac (float):
        The fraction of the data to use for training. The remaining data will be
        returned as testing data.
      window_size (int):
        The number of data values in the rolling lookback window.

    Returns:
      train_dict (dict):
        A dict with a Pandas DataFrame of the data for training in input format 
        inside its "data" field.
      test_dict (dict):
        A dict with a Pandas DataFrame of the data for testing in input format 
        inside its "data" field.
      train_window_dict (dict):
        A dict of the data for training in the "data" field, with a list of 
        lookback windows in the "windows" field and a list of the corresponding 
        next values in the "next_vals" field.
      test_window_dict (dict):
        A dict of the data for testing in the "data" field, with a list of 
        lookback windows in the "windows" field and a list of the corresponding 
        next values in the "next_vals" field.
    """

    # Load data into dataframe
    df = pd.DataFrame.from_dict(data)
    print("DataFrame head:")
    print(df.head())

    dfsize = df.shape[0]

    # Splitting up dataset into Training and Testing datsets
    train_size = int(dfsize * train_frac)
    test_size = dfsize - train_size
    train, test = df.iloc[0:train_size], df.iloc[train_size:]

    print("Train and test set sizes: ", len(train), len(test))

    # Reshape to dimensions required by tensorflow: [samples, window_size, n_features]
    col = df.columns[1]
    train_windows, train_next_vals = create_dataset(train[col], train[col], window_size)
    test_windows, test_next_vals = create_dataset(test[col], test[col], window_size)

    # Save all 4 data sets to JSON serializable formats (dicts/lists)
    train_dict = {}
    train_dict["data"] = train.to_dict()

    test_dict = {}
    test_dict["data"] = test.to_dict()

    train_window_dict = {"data":{}}
    train_window_dict["data"]["windows"] = train_windows.tolist()
    train_window_dict["data"]["next_vals"] = train_next_vals.tolist()

    test_window_dict = {"data":{}}
    test_window_dict["data"]["windows"] = test_windows.tolist()
    test_window_dict["data"]["next_vals"] = test_next_vals.tolist()

    return train_dict, test_dict, train_window_dict, test_window_dict


def create_dataset(x: pd.Series, y: pd.Series, window_size:int=1) -> (np.ndarray, np.ndarray):
    """
    A helper function of `preprocess_data` to split data into lookback windows 
    and next values.

    Args:
      x (pd.Series):
        The data to make the lookback windows from
      y (pd.Series):
        The data to get the next values from
      window_size (int):
        The size of the lookback window.

    Returns:
      np.array(xs) (numpy.ndarray):
        An array of lookback windows.
      np.array(ys) (numpy.ndarray):
        An array of corresponding next values.
    """

    xs, ys = [], []

    # Create pairs of a window of data and the next value after the window
    for i in range(len(x) - window_size):
        v = x.iloc[i:(i + window_size)].values
        xs.append(v)
        ys.append(y.iloc[i + window_size])

    return np.array(xs), np.array(ys)


def save_data(datas:list, filenames:list) -> None:
    """
    Saves data as JSON.

    Args:
      datas (list):
        A list of dicts of data to save.
      filenames (list):
        A list of filenames corresponding to the data dicts to save the data in. 
        These should have a '.json' extension.
    """

    for i in range(len(datas)):
        data = datas[i]
        filename = filenames[i]

        try:
            data["schema"] = "orquestra-v1-data"
        except KeyError as e:
            print(f'Error: Could not load schema key from {filename}')

        try:
            with open(filename,'w') as f:
                # Write data to file as this will serve as output artifact
                f.write(json.dumps(data, indent=2)) 
        except IOError as e:
            print(f'Error: Could not open {filename}')


def load_data(filename:TextIO) -> dict:
    """
    Loads data from JSON.

    Args:
      filename (TextIO):
        The file to load the data from.

    Returns:
      data (dict):
        The data that was loaded from the file.
    """

    if isinstance(filename, str):
        try:
            with open(filename, 'r') as f:
                data = json.load(f)

        except IOError:
            print(f'Error: Could not open {filename}')

    else:
        data = json.load(filename)

    return data
```

- `lstm_model.py`:

```python
import json
import pandas as pd
import numpy as np

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential, model_from_json, load_model, save_model
from tensorflow.keras import callbacks


def build_model(data:dict, hnodes:int=32, dropout:float=0.2) -> Sequential:
    """
      Builds LSTM model with an LSTM layer, dropout layer, and dense layer.

      Args:
        data (dict):
          A dict of data to use the input shape of to build the model.
          It must have two keys, each containing indexes as keys and data as 
          values (this is the dict format of Pandas DataFrames). Here is an 
          example:
          {
            "x": {
              "0": 0.0,
              "1": 0.1
            },
            "y": {
              "0": 1.0,
              "1": 2.0
            }
          }
        hnodes (int):
          The number of nodes in the LSTM layer.
        dropout (float):
          The fraction of the LSTM layer to be the dropout layer.

      Returns:
        model (keras.models.Sequential):
          The model that was built.
    """

    # Load data into dataframe
    df = pd.DataFrame.from_dict(data)
    print("DataFrame head:")
    print(df.head())

    # Add a '1' as the third dimension of the DataFrame shape if it's not there
    if len(df.shape) != 3:
        df = np.expand_dims(df, axis=2)
    print("Shape of input data: ", df.shape)

    print("DataFrame shape:")
    print(df.shape)

    # Gets size of window from length of array in dataframe row 0 col 0
    window_size = len(df[0][0][0])

    model = keras.Sequential()

    # Adding one LSTM layer
    model.add(keras.layers.LSTM(
        units=hnodes,
      input_shape=(window_size, df.shape[2])
    ))

    # Adding Dropout
    model.add(keras.layers.Dropout(dropout))

    # Adding a Dense layer at the end
    model.add(keras.layers.Dense(units=1))

    return model


def train_model(model:Sequential, data:dict, nepochs:int=30, batchsize:int=32, valsplit:float=0.1, learning_rate:float=0.01) -> (callbacks, Sequential):
    """
    Trains the input model using input data.

    Args:
      model (Sequential):
        The model to train.
      data (dict):
        The data to train the model on. This should be in a dict with keys 
        "windows and "next_vals", where "windows" contains a list of lookback 
        windows and "next_vals" contains a list of the corresponding next values.
      nepochs (int):
        The number of training epochs to perform.
      batchsize (int):
        The batch size for training.
      valsplit (float):
        The fraction of the data to use for validation during training.
      learning_rate (float):
        The learning rate for training.

    Returns:
      fithistory.history (keras.History.history):
        The keras history object from training.
      model (Sequential):
        The trained model.
    """
    try:
        windows = np.array(data["windows"])
        next_vals = np.array(data["next_vals"])
    except KeyError:
        print(f'Error: Could not load windows and next_vals from data.')

    # Add a '1' as the third dimension of the data shape if it's not there
    if len(windows.shape) == 2:
        windows = np.expand_dims(windows, axis=2)
    print("Shape of input windows: ", windows.shape)

    model.compile(
        loss='mean_squared_error',
      optimizer=keras.optimizers.Adam(learning_rate)
    )

    fithistory = model.fit(
        windows, next_vals,
      epochs=nepochs,
      batch_size=batchsize,
      validation_split=valsplit,
      verbose=1,
      shuffle=False
    )

    return fithistory.history, model


def predict(model:Sequential, data:dict) -> dict:
    """
    Makes predictions about input data using input model.

    Args:
      model (Sequential):
        The model to use for predictions.
      data (dict):
        The data to make predictions about. This should be in a dict with keys 
        "windows and "next_vals", where "windows" contains a list of lookback 
        windows and "next_vals" contains a list of the corresponding next values.

    Returns:
      pred_dict (dict):
        A dict with a list of predictions in the "data" field.
    """

    windows = np.array(data["windows"])

    # Add a '1' as the third dimension of the data shape if it's not there
    if len(windows.shape) == 2:
        windows = np.expand_dims(windows, axis=2)
    print("Shape of input windows: ", windows.shape)

    pred = model.predict(windows)

    # Save predictions to a JSON serializable format (a dict of a list)
    pred_dict = {}
    pred_dict["data"] = pred.tolist()

    return pred_dict


def save_model_json(model:Sequential, filename:str) -> None:
    """
    Saves a model's architecture and weights as a JSON file. The output JSON will 
    contain a "model" field with "specs" and "weights" fields inside it. The 
    "specs" field contains the model's architecture and the "weights" field
    contains the weights.

    Args:
      model (keras.models.Sequential):
        The model to save.
      filename (str):
        The name of the file to save the model in. This should have a '.json'
        extension.
    """

    model_dict = {"model":{}}

    model_json = model.to_json()
    model_dict["model"]["specs"] = json.loads(model_json)

    weights = model.get_weights()
    # Convert weight arrays to lists because those are JSON compatible
    weights = nested_arrays_to_lists(weights)
    model_dict["model"]["weights"] = weights

    model_dict["schema"] = "orquestra-v1-model"

    try:
        with open(filename, "w") as f:
            f.write(json.dumps(model_dict, indent=2))
    except IOError:
        print('Error: Could not load {filename}')


def load_model_json(filename:str) -> Sequential:
    """
    Loads a keras model from a JSON file.

    Args:
      filename (str):
        The JSON file to load the model from. This file must contain a "model" 
        field with "specs" and "weights" fields inside it. The "specs" field 
        should contain the model's architecture and the "weights" field should
        contain the weights. (The format saved by the `save_model_json` function.)

    Returns:
      model (Sequential):
        The model loaded from the file. 
    """

    # load json and create model
    with open(filename) as json_file:
        loaded_model_artifact = json.load(json_file)

    loaded_model = json.dumps(loaded_model_artifact["model"]["specs"])
    loaded_model = model_from_json(loaded_model)

    try:
        weights = loaded_model_artifact["model"]["weights"]
    except KeyError:
        print(f'Error: Could not load weights from {weights}')

    # Everything below the top-level list needs to be converted to a numpy array
    # because those are the types expected by `set_weights`
    for i in range(len(weights)):
        weights[i] = nested_lists_to_arrays(weights[i])
    loaded_model.set_weights(weights)

    return loaded_model


def nested_arrays_to_lists(obj):
    """
    Helper function for saving models in JSON format. Converts nested numpy 
    arrays to lists (lists are JSON compatible).
    """

    if isinstance(obj, np.ndarray):
        obj = obj.tolist()

    try:
        for i in range(len(obj)):
            obj[i] = nested_arrays_to_lists(obj[i])

    except TypeError:
        return obj

    return obj

def nested_lists_to_arrays(obj):
    """
    Helper function for loading models in JSON format. Converts nested lists to numpy arrays (numpy arrays are the expected type to set the weights 
    of a Keras model).
    """

    if isinstance(obj, list):
        obj = np.array(obj)

    try:
        for i in range(len(obj)):
            obj[i] = nested_lists_to_arrays(obj[i])
    except TypeError:
        return obj

    return obj


def save_model_h5(model:Sequential, filename:str) -> None:
    """
    Saves a complete model as an H5 file. H5 files can be used to pass models 
    between tasks but cannot be returned in a workflowresult.

    Args:
      model (keras.models.Sequential):
        The model to save.
      filename (str):
        The name of the file to save the model in. This should have a '.h5'
        extension.
    """
    keras.models.save_model(
        model, filename, include_optimizer=True
    )


def load_model_h5(filename:str) -> Sequential:
    """
    Loads a keras model from an H5 file. H5 files can be used to pass models 
    between tasks but cannot be returned in a workflowresult.

    Args:
      filename (str):
        The H5 file to load the model from. This should have the format created 
        by `keras.models.save_model`.

    Returns:
      model (Sequential):
        The model loaded from the file. 
    """

    model = keras.models.load_model(filename, compile=True)
    return model

def save_loss_history(history, filename:str) -> None:
    """
    Saves a keras.History.history object to a JSON file.

    Args:
      history (keras.History.history):
        The history object to save.
      filename (str):
        The name of the file to save the history in. This should have a '.json'
        extension.
    """

    history_dict = {}
    history_dict["history"] = history
    history_dict["schema"] = "orquestra-v1-loss-function-history"

    try:
        with open(filename, "w") as f:
            f.write(json.dumps(history_dict, indent=2))
    except IOError:
        print(f'Could not write to {filename}')
```

**3. Adding a `setup.py`**

Create a file `src/setup.py` with the following contents:

```python
import os
import setuptools

readme_path = os.path.join("..", "README.md")
with open(readme_path, "r") as f:
    long_description = f.read()

setuptools.setup(
    name                            = "z-lstm",
    version                         = "0.1.0",
    author                          = "Zapata Computing, Inc.",
    author_email                    = "info@zapatacomputing.com",
    description                     = "Prediction with LSTM for Orquestra.",
    long_description                = long_description,
    long_description_content_type   = "text/markdown",
    url                             = "https://github.com/zapatacomputing/z-lstm",
    packages                        = setuptools.find_packages(where = "python"),
    package_dir                     = {"" : "python"},
    classifiers                     = (
        "Programming Language :: Python :: 3",
        "Operating System :: OS Independent",
    ),
    install_requires = [
        "tensorflow",
        "pandas",
        "numpy"
   ],
)
```

**4. Commit and push your component**

Commit your changes and push them to GitHub (please note that you will not need to do this if you are using the GitHub UI to modify the repository).
The structure of your repository should look like this:
```
.
└── src
   ├── python/lstm
   │   ├── data_manipulator.py
   │   └── lstm_model.py
   └── setup.py
```

**5. Building a Workflow**

Create file `lstm-tutorial.zqwl` (with code shown below), insert the URL of your GitHub repository on line 15. This file can go anywhere, and in the repo, you'll find it under `example`.

This zqwl file orchestrates all the different steps corresponding to data generation and preprocessing, and the building up and training of the model, ending up with running the model prediction on the test dataset.

- `lstm-tutorial.zqwl`:

# TODO: Update this to v1
```YAML
# Workflow API version
ZapOSApiVersion: v1alpha1

# Declares this as workflow
kind: Workflow

# List resources needed by workflow.
resources:

# A resource named `z-lstm` that is a public git repo. All the fields here are required except branch, which defaults to master.
- name: z-lstm
  type: git
  parameters:
    url: "git@github.com:<your-github-username>/<your-git-repo-name>.git"
    branch: "master"

# Data to help you easily work with your workflow
metadata:

  # Prefix for workflow ID
  generateName: lstm-tutorial-

# Data for running the workflow
spec:

  # Think of this as identifying the `main` function -- this tells the workflow which template to start with
  entrypoint: lstm

  # Initializing global variables for use in workflow
  arguments:
    parameters:

    # Where output data is stored -- Must be `quantum-engine` for compatibility with Orquestra data services
    - s3-bucket: quantum-engine
    # Path where output data is stored within the `s3-bucket` -- can be anything you want
    - s3-key: tutorials/lstm/

  # The steps of the workflow
  templates:

  # `lstm` is a template that just contains a list of `steps`, which are other templates
  - name: lstm
    steps:

    # This template runs the `generate-data` template in the `hello` resource
    - - name: generate-data
        template: generate-data
        arguments:
          parameters:
          - resources: [z-lstm]
          - docker-image: z-ml
          - docker-tag: latest
          - time-range: "100"
          - time-step: "0.1"
          - noise-std: "0.1"
    - - name: preprocess-data
        template: preprocess-data
        arguments:
          parameters:
          - resources: [z-lstm]
          - docker-image: z-ml
          - docker-tag: latest
          - train-frac: "0.8"
          - window-size: "10"
          artifacts:
          - name: data
            from: '{{steps.generate-data.outputs.artifacts.data}}'
    - - name: build-model
        template: build-model
        arguments:
          parameters:
          - resources: [z-lstm]
          - docker-image: z-ml
          - docker-tag: latest
          - hnodes: "128"
          - dropout: "0.1"
          artifacts:
          - name: preprocessed-data
            from: '{{steps.preprocess-data.outputs.artifacts.testing-data-windows}}'
    - - name: train-model
        template: train-model
        arguments:
          parameters:
          - resources: [z-lstm]
          - docker-image: z-ml
          - docker-tag: latest
          - nepochs: "100"
          - batch-size: "32"
          - val-split: "0.1"
          - learning-rate: "0.01"
          artifacts:
          - name: model
            from: '{{steps.build-model.outputs.artifacts.model}}'
          - name: training-data
            from: '{{steps.preprocess-data.outputs.artifacts.training-data-windows}}'
    - - name: predict-using-model
        template: predict-using-model
        arguments:
          parameters:
          - resources: [z-lstm]
          - docker-image: z-ml
          - docker-tag: latest
          artifacts:
          - name: model
            from: '{{steps.train-model.outputs.artifacts.trained-model}}'
          - name: testing-data
            from: '{{steps.preprocess-data.outputs.artifacts.testing-data-windows}}'
```

**7. Running the Workflow**

Now all should be ready to run the workflow.

* Make sure you have installed the [Quantum Engine CLI](../../qe-cli/install-cli/)

* Log in to Quantum Engine by running `qe login -e <your-email> -s <quantum-engine-uri>` in your terminal. Contact support to register your email and/or receive the `quantum-engine-uri`.

* Submit your `lstm-tutorial.zqwl` by running `qe submit workflow <path/to/workflow/lstm-tutorial.zqwl>`

This will return the workflow ID that corresponds to that particular execution of your workflow. The output will look like:
```Bash
Successfully submitted workflow to quantum engine!
Workflow ID: lstm-tutorial-6m248
```

**8. Workflow Progress**

The workflow is now submitted to the Orquestra Quantum Engine and will be scheduled for execution when compute becomes available.

To see details of the execution of your workflow, run `qe get workflow <workflow-ID>` with the workflow ID from the previous step.

 The output will look like:

![](../../img/tutorials/WorkflowLSTM.png)

**9. Workflow Results**

To get the results of your workflow, run `qe get workflowresult <workflow-ID>` with your workflow ID.

After a workflow runs, it takes time for the data to be processed. This results file cannot be created until the data is done being processed. You can try running the above command every few minutes until it returns a link to download the result file.

Once finished, the output will look like the following:
```Bash
Name:        lstm-tutorial-6m248
Location:    http://40.89.254.33:9000/workflow-results/a746f35c-f00a-513a-87f4-d2e4ddc0d9b5.json?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=zapata%2F20200514%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200514T213402Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D%22a746f35c-f00a-513a-87f4-d2e4ddc0d9b5.json%22&X-Amz-Signature=a4aa2d1a7db1c9be556054683fee0281de54e852f4624964ac86ba5e2174f207
```
___


## Plotting the results

To plot the result from the training and testing processes, run `plot_lstm.py`, which can be found inside the `example` directory. This file plots the results from the existing final `JSON` file generated by the workflow above, and it can be run passing the JSON file as a parameter after the python script, i.e., `python3 plot_lstm.py <you_own_json_file>`. 

- `plot_lstm.py`:

```python
import sys
import json
import pandas as pd
from matplotlib import pyplot as plt

def plot(filename):
    # Insert the path to your JSON file here
    try:
        with open(filename) as f:
            results = json.load(f)
    except IOError:
        print(f'Error: {filename} not found.')

    training_loss_vals = []
    validation_loss_vals = []
    predicted_vals = []

    for step in results:
        if results[step]['class'] == 'train-model':
            training_loss_vals = results[step]['history']['history']['loss']
            validation_loss_vals = results[step]['history']['history']['val_loss']
        if results[step]['class'] == 'preprocess-data':
            training_df = pd.DataFrame(results[step]['training-data']['data'])
            testing_df = pd.DataFrame(results[step]['testing-data']['data'])
        if results[step]['class'] == 'predict-using-model':
            predicted_vals_obj = results[step]['predictions']['data']
            for entry in predicted_vals_obj:
                predicted_vals.append(entry['data'][0]['data'])

    # Converting indices to ints and sorting by indices
    training_df.index = training_df.index.astype(int)
    training_df.sort_index(inplace=True)
    testing_df.index = testing_df.index.astype(int)
    testing_df.sort_index(inplace=True)

    # Plotting values from training process
    plt.figure()
    plt.plot(training_loss_vals, label='Train')
    plt.plot(validation_loss_vals, label='Validation')
    plt.xlim(left=0.0)
    plt.ylim(bottom=0.0)
    plt.grid()
    plt.legend()
    plt.title("Loss function: MSE")
    plt.xlabel('Epoch')
    plt.ylabel('Loss function value')

    # Plotting results: train, test, and predicted datasets
    plt.figure()

    plt.plot(training_df['time'], training_df['values'], color='g', label="Training", zorder=2)
    plt.plot(testing_df['time'], testing_df['values'], color='b', label="Testing", zorder=3)
    plt.scatter(testing_df['time'].values[10:], predicted_vals, marker='o', s=15., color='r', label="Predicted", zorder=4)

    plt.ylabel('Value')
    plt.xlabel('Time')
    plt.legend()
    plt.grid()
    plt.title("Training, Test and Predicted datasets")

    plt.show()


if __name__ == '__main__':
    try:
        filename = sys.argv[1]
    except IndexError:
        print(f'Error: please pass the JSON result file to be plotted.')
        sys.exit(1)

    plot(filename)
```

As an example of the expected plotting output, please see the graph below representing the Training and Testing datasets as well as the values corresponding to the prediction phase by the trained model (plotted as overlapping points):

![](../../img/tutorials/TrainingTestPredict.png)

Finally, zooming in on the last part of the plot above shows just a window of the whole dataset focusing on the test data and predicted values.

![](../../img/tutorials/TestPred.png)
