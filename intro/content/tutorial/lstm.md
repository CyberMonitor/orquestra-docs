---
title: 4.  Prediction with LSTM
description:  Prediction of a sine function by means of Recurrent Neural Networks
---

## Introduction

This tutorial will walk through an implementation of a neural network model that is able to learn how to generate a sine function using *Recurrent Neural Networks*.

A recurrent neural network  (RNN) deals with sequence problems as they can retain the state from one iteration to the next by using their own output as additional input for the next step, i.e. in a RNN it is given the network the data and the state of the network one previous step. A simple recurrent network suffers from a fundamental problem of not being able to capture long-term dependencies in a sequence, and in order to tackle that issue in late ’90s, *LSTM* was proposed by **Sepp Hochreiter** and **Jurgen Schmidhuber** (https://www.mitpressjournals.org/doi/abs/10.1162/neco.1997.9.8.1735). This exercise will make used of LSTMs as neurons type in the model.

The problem here considered belongs in the category of *Supervised Learning*, which means learning a function to maps a set of inputs to one of outputs based on multiple instances of (input, output) pairs.

## Neural Network Model
The RNN model may be considered a computational black box that takes in a number of sequential values within a lookback window of values and forecast the next value. To this aim the model consists of a neural network with a single hidden layer of LSTM neurons and a final *Dense* layer for the output. Additionally, in order to prevent overfitting a Dropout operation is added after the LSTM layer.

In order for the model to learn it is required to specify a *Loss function*, to measure the deviation between the predicted values of the model and the real ones, and an *Optimizer* as the computational object to drive the search aims to minimize that deviation. As loss function it is used a mean squared error (MSE), and an extension of stochastic gradient descent, Adam, as optimizer. Below is the MSE loss function, and the snippet of the code to build the model.

![](../img/MSE_LossFunction.png)


```python
# Function to build the model
def build_model(hnodes=32, dropout=0.2):
    model = keras.Sequential()

    # Adding one LSTM layer
    model.add(keras.layers.LSTM(
        units=hnodes,
        input_shape=(xtrain.shape[1], xtrain.shape[2])
    ))

    # Adding Dropuut
    model.add(keras.layers.Dropout(dropout))

    # Adding a Dense layer at the end
    model.add(keras.layers.Dense(units=1))

    # Compile model using MSE as loss function to minimize, and Adam as optimiser
    model.compile(
        loss='mean_squared_error',
        optimizer=keras.optimizers.Adam(learning_rate)
    )
```
The summarized model output of the code above reads as follows:

![](../img/ModelPlot.png)

The model may save its architecture and weights as a JSON file, which enables it to be utilized a posteriori as a black-box predictor, or even as initialization condition for further training, for instance in case of *transfer learning*.

## Input Dataset
The input dataset consists of a noisy sine data, generated by adding up an analytical sine function and a noisy signal sampled from a normal distribution with mean zero and standard deviation of one. The parameters that controls the data generation are the folowing:
- *time-range*: the upper limit of the time range for the data to generate. The time range starts at 0. The time_range is not included as the last point.
- *time-step*: the step between each of the time values.
- *noise-std*: the standard deviation of the noise. Noise follows a normal distribution centered at zero.
    
![](../img/SineFunc.png)


## Test-Train Split

To train the model the full original dataset is first divided into training and testing sets, where the first dataset will be used for training the model, and the second one to check the generalization capability of the model. In general terms:

- *Train dataset* is used to train the model, i.e. weights and biases in the LSTM layers. The model learns from this data.

- *Valuation dataset* it is a percentage of the training dataset used to provide an unbiased evaluation of the model along the training process, and it is a means to assess a possible overfitting of the model. This percentage value is a hyperparameter of the model, which in our example is initially set up to 0.1, i.e. 10% of the training data.

- *Test dataset* is used to provide an unbiased evaluation of the model. It is only used once a model is completely trained.

![](../img/TrainTest.png)


After splitting the data, both training and testing datasets are subjected to an operation that create the pair values x and y of a supervised problem. On this respect, the x-values correspond to a time rolling window of specified length, and the y-values to the next value to be predicted. Below there is a snippter of that code.

```python

  # Create pairs of a window of data and the next value after the window
  xs, ys = [], []
  for i in range(len(x) - window_size):
      v = x.iloc[i:(i + window_size)].values
      xs.append(v)
      ys.append(y.iloc[i + window_size])

    return np.array(xs), np.array(ys)
```

## Plotting Prediction vs Test values
Below it is a plot representing the training history, i.e. the Loss function (MSE), defined at the time of model compilation above, for both during validation and training. It can be noted that the validation error is less than the training error, which can be understood as the training error is calculated for the entire epoch - where at its begining is worse than at the end -, whereas the validation error is taken from the last batch  -after the model is improved by the traqining process.

![](../img/CostFunction.png)


## Composing a workflow to generate a LSTM predictor model
In the next steps it will be stated the code necessary to build and train this RNN model in Orquestra. The code consists of the following main parts:
- `generate-data.yaml`: generates the dataset and runs the entire workflow.
- `data_manipulator.py`: builds and pre-processes the data.
- `lstm_model.py`: builds and trains the Network model.

**1. Create a GitHub repository**

Go to [GitHub](https://github.com/) and create a public repository called `lstm`. If you are unfamiliar with GitHub you can reference their [create a repo guide](https://help.github.com/en/github/getting-started-with-github/create-a-repo) for help

This repository will be where you build your resource. [This GitHub repo](https://github.com/zapatacomputing/z-lstm) can be used as a reference for how the `lstm` resource should look like throughout the tutorial.

**2. Add python code to the repository**
Now we will add the bulk of the workflow, which is the Python code that trains the model. Two files are required

Using either the GitHub UI or by cloning your repo and using the command line, create a file `src/python/lstm/lstm_model.py`.

**3. Adding a `setup.py`**

Create a file `src/setup.py` with the following contents:

```python
import os
import setuptools

readme_path = os.path.join("..", "README.md")
with open(readme_path, "r") as f:
    long_description = f.read()

setuptools.setup(
    name                            = "z-lstm",
    version                         = "0.1.0",
    author                          = "Zapata Computing, Inc.",
    author_email                    = "info@zapatacomputing.com",
    description                     = "Prediction with LSTM for Orquestra.",
    long_description                = long_description,
    long_description_content_type   = "text/markdown",
    url                             = "https://github.com/zapatacomputing/z-lstm",
    packages                        = setuptools.find_packages(where = "python"),
    package_dir                     = {"" : "python"},
    classifiers                     = (
        "Programming Language :: Python :: 3",
        "Operating System :: OS Independent",
    ),
    install_requires = [
        "tensorflow",
        "pandas",
        "numpy"
   ],
)
```
**4. Adding `templates`**

Create a file `templates/.yaml` with the following contents:


**5. Commit and push your resource**

Commit your changes and push them to GitHub.
(Note that you will not need to do this if you are using the GitHub UI to modify the repository.)
The structure of your repository should look like this:
```
.
├── src
│   ├── python/lstm
│   │   ├── data_manipulator.py
│   │   └── lstm_model.py
│   └── setup.py
└── templates
    └── generate-data.yaml
    └── preprocess-data.yaml
    └── build-model.yaml
    └── train-model.yaml
```

**6. Building a Workflow**

Create a file `lstm-tutorial.yaml` file with the code below, inserting the URL of your github repository in line 15. This file can go anywhere (except the `templates` folder), and in the repo you'll find it under `examples`.

This yaml file orquestrate all the different steps corresponding to data generation and preprocessing, and the buiding uo and training of the model, ending up with running the model prediction for test dataset.

```YAML
# Workflow API version
ZapOSApiVersion: v1alpha1

# Declares this as workflow
kind: Workflow

# List resources needed by workflow.
resources:

# A resource named `z-lstm` that is a public git repo. All the fields here are required except branch, which defaults to master.
- name: z-lstm
  type: git
  parameters:
    url: "git@github.com:<your-github-username>/<your-git-repo-name>.git"
    branch: "master"

# Data to help you easily work with your workflow
metadata:

  # Prefix for workflow ID
  generateName: lstm-tutorial-

# Data for running the workflow
spec:

  # Think of this as identifying the `main` function -- this tells the workflow which template to start with
  entrypoint: lstm

  # Initializing global variables for use in workflow
  arguments:
    parameters:

    # Where output data is stored -- Must be `quantum-engine` for compatibility with Orquestra data services
    - s3-bucket: quantum-engine
    # Path where output data is stored within the `s3-bucket` -- can be anything you want
    - s3-key: tutorials/lstm/

  # The steps of the workflow
  templates:

  # `lstm` is a template that just contains a list of `steps`, which are other templates
  - name: lstm
    steps:

    # This template runs the `generate-data` template in the `hello` resource
    - - name: generate-data
        template: generate-data
        arguments:
          parameters:
          - resources: [z-lstm]
          - docker-image: z-ml
          - docker-tag: latest
          - time-range: "100"
          - time-step: "0.1"
          - noise-std: "0.1"
    - - name: preprocess-data
        template: preprocess-data
        arguments:
          parameters:
          - resources: [z-lstm]
          - docker-image: z-ml
          - docker-tag: latest
          - train-frac: "0.8"
          - window-size: "10"
          artifacts:
          - name: data
            from: '{{steps.generate-data.outputs.artifacts.data}}'
    - - name: build-model
        template: build-model
        arguments:
          parameters:
          - resources: [z-lstm]
          - docker-image: z-ml
          - docker-tag: latest
          - hnodes: "128"
          - dropout: "0.1"
          artifacts:
          - name: preprocessed-data
            from: '{{steps.preprocess-data.outputs.artifacts.testing-data-windows}}'
    - - name: train-model
        template: train-model
        arguments:
          parameters:
          - resources: [z-lstm]
          - docker-image: z-ml
          - docker-tag: latest
          - nepochs: "100"
          - batch-size: "32"
          - val-split: "0.1"
          - learning-rate: "0.01"
          artifacts:
          - name: model
            from: '{{steps.build-model.outputs.artifacts.model}}'
          - name: training-data
            from: '{{steps.preprocess-data.outputs.artifacts.training-data-windows}}'
    - - name: predict-using-model
        template: predict-using-model
        arguments:
          parameters:
          - resources: [z-lstm]
          - docker-image: z-ml
          - docker-tag: latest
          artifacts:
          - name: model
            from: '{{steps.train-model.outputs.artifacts.trained-model}}'
          - name: testing-data
            from: '{{steps.preprocess-data.outputs.artifacts.testing-data-windows}}'
```

**7. Running the Workflow**

You are now ready to run the workflow!

* Make sure you have installed the [Quantum Engine CLI](https://www.orquestra.io/docs/qe/cli/install/)

* Log in to Quantum Engine by running `qe login -e <your-email> -s <quantum-engine-uri>` in your terminal. Contact support to register your email and/or receive the `quantum-engine-uri`.

* Submit your `lstm-tutorial.yaml` by running `qe submit workflow <path/to/workflow/lstm-tutorial.yaml>`

This will return the workflow ID that corresponds to that particular execution of your workflow. The output will look like:
```Bash
Successfully submitted workflow to quantum engine!
Workflow ID: lstm-tutorial-6m248
```

**8. Worfklow Progress**

The workflow is now submitted to the Orquestra Quantum Engine and will be scheduled for execution when compute becomes available.

To see details of the execution of your workflow, run `qe get workflow <workflow-ID>` with your workflow ID from the previous step substituted in.

 The output will look like:

![](../img/WorkflowLSTM.png)

**9. Workflow Results**

To get the results of your workflow, run `qe get workflowresult <workflow-ID>` with your workflow ID.

After a workflow runs, it takes time for the data to be processed. This results file cannot be created until the data is done being processed. You can try running the above command every few minutes until it returns a link to download a file.

Once finished, the output will look like the following:
```Bash
Name:        lstm-tutorial-6m248
Location:    http://40.89.254.33:9000/workflow-results/a746f35c-f00a-513a-87f4-d2e4ddc0d9b5.json?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=zapata%2F20200514%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200514T213402Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3D%22a746f35c-f00a-513a-87f4-d2e4ddc0d9b5.json%22&X-Amz-Signature=a4aa2d1a7db1c9be556054683fee0281de54e852f4624964ac86ba5e2174f207
```
___


## Plotting the results

In order to plot the training process, we use the file `plot_qcbm_opt_history.py`. In the repo, you can find it in the folder named `examples`. This plots the results from the existing `qcbm_example.json` file, but if you'd like to plot the results of your own training, please change the name to your json file in line 22. 

```python
import sys
import json
import pandas as pd
from matplotlib import pyplot as plt

def plot(filename):
    # Insert the path to your JSON file here
    try:
        with open(filename) as f:
            results = json.load(f)
    except IOError:
        print(f'Error: {filename} not found.')

    training_loss_vals = []
    validation_loss_vals = []
    predicted_vals = []

    for step in results:
        if results[step]['class'] == 'train-model':
            training_loss_vals = results[step]['history']['history']['loss']
            validation_loss_vals = results[step]['history']['history']['val_loss']
        if results[step]['class'] == 'preprocess-data':
            training_df = pd.DataFrame(results[step]['training-data']['data'])
            testing_df = pd.DataFrame(results[step]['testing-data']['data'])
        if results[step]['class'] == 'predict-using-model':
            predicted_vals_obj = results[step]['predictions']['data']
            for entry in predicted_vals_obj:
                predicted_vals.append(entry['data'][0]['data'])

    # Converting indices to ints and sorting by indices
    training_df.index = training_df.index.astype(int)
    training_df.sort_index(inplace=True)
    testing_df.index = testing_df.index.astype(int)
    testing_df.sort_index(inplace=True)

    # Plotting values from training process
    plt.figure()
    plt.plot(training_loss_vals, label='Train')
    plt.plot(validation_loss_vals, label='Validation')
    plt.xlim(left=0.0)
    plt.ylim(bottom=0.0)
    plt.grid()
    plt.legend()
    plt.title("Loss function: MSE")
    plt.xlabel('Epoch')
    plt.ylabel('Loss function value')

    # Plotting results: train, test and perdicted datasets
    plt.figure()

    plt.plot(training_df['time'], training_df['values'], color='g', label="Training", zorder=2)
    plt.plot(testing_df['time'], testing_df['values'], color='b', label="Testing", zorder=3)
    plt.scatter(testing_df['time'].values[10:], predicted_vals, marker='o', s=15., color='r', label="Predicted", zorder=4)

    plt.ylabel('Value')
    plt.xlabel('Time')
    plt.legend()
    plt.grid()
    plt.title("Training, Test and Predicted datasets")

    plt.show()


if __name__ == '__main__':
    try:
        filename = sys.argv[1]
    except IndexError:
        print(f'Error: please pass the JSON result file to be plotted.')
        sys.exit(1)

    plot(filename)
```

![](../img/TrainingTestPredict.png)

Below shows the last part of the above plot representing the Testing dataset and its corresponding Prediction by the model.

![](../img/TestPred.png)
